{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic regression for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"burk\">虽然名字中含有Regression，但是其为分类算法，离散化输出。</span>  \n",
    "<span class=\"mark\">当然他也可以做回归，拟合曲线，只不过预测值predict不使用threshold，所以输出是连续的，不是离散的。</span>  \n",
    "为什么不用linear Regression做分类：线性回归拟合直线，使用threshold对预测值进行划分的分类算法做分类不是很好的算法。因为数据本身的特性可能不是一条直线，很多情况下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sigmoid function or logistic function  \n",
    "这是为什么叫做logistic regression的原因：   \n",
    "\\begin{align*}& h_\\theta (x) = g ( \\theta^T x ) \\newline \\newline& z = \\theta^T x \\newline& g(z) = \\dfrac{1}{1 + e^{-z}}\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二分类问题理解：实质上逻辑回归在数据集上做线性划分，在线的一侧的样本z(x)大于0，另一侧小于0.在logistic function中，正是以0作为中心点，分为大于0.5和小于0.5的两部分。由于输出值在（0,1）之间，很自然就联想到概率可能性。 \n",
    "\n",
    "线性：Θ‘x = 0代表decision boundary，决策边界。当x = [1 x1 x2]的形式出现时，则其为一条直线，将样本分开。  \n",
    "非线性：当x = [1 x1^2 x2^2]的形式出现时，其为一个圆，将样本分开。  \n",
    "<span class=\"mark\">当然特征的形式可以组合得更加复杂，则决策边界也更加奇怪有趣。</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4S4ikoP+P+A+XRt30Lt4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f33165bbb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(-10, 10, 0.1)\n",
    "y = 1./(1 + np.e**(-x))\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logistic Regression cost function：\n",
    "\\begin{align*}& J(\\theta) = \\dfrac{1}{m} \\sum_{i=1}^m \\mathrm{Cost}(h_\\theta(x^{(i)}),y^{(i)}) \\newline & \\mathrm{Cost}(h_\\theta(x),y) = -\\log(h_\\theta(x)) \\; & \\text{if y = 1} \\newline & \\mathrm{Cost}(h_\\theta(x),y) = -\\log(1-h_\\theta(x)) \\; & \\text{if y = 0}\\end{align*} \n",
    "首先L(x)>=0,并且偏离越大，惩罚越大。  \n",
    "\\begin{align*}& \\mathrm{Cost}(h_\\theta(x),y) = 0 \\text{ if } h_\\theta(x) = y \\newline & \\mathrm{Cost}(h_\\theta(x),y) \\rightarrow \\infty \\text{ if } y = 0 \\; \\mathrm{and} \\; h_\\theta(x) \\rightarrow 1 \\newline & \\mathrm{Cost}(h_\\theta(x),y) \\rightarrow \\infty \\text{ if } y = 1 \\; \\mathrm{and} \\; h_\\theta(x) \\rightarrow 0 \\newline \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span class=\"mark\">优化对损失函数的要求</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数是凸函数时，优化的结果收敛于全局最优。否则收敛于局部最优。  \n",
    "<span class=\"burk\">怎么才能保证损失函数是convex function?</span>  \n",
    "<span class=\"burk\">逻辑回归损失函数为什么是凸函数？</span>  \n",
    "Note that writing the cost function in this way guarantees that J(θ) is convex for logistic regression.  \n",
    "[为什么logistic Cost function is convex？](http://mathgotchas.blogspot.com/2011/10/why-is-error-function-minimized-in.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent更新方式推导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*} & J(\\theta) = - \\frac{1}{m} \\displaystyle \\sum_{i=1}^m [y^{(i)}\\log (h_\\theta (x^{(i)})) + (1 - y^{(i)})\\log (1 - h_\\theta(x^{(i)}))] \\newline & h = g(X\\theta) \\newline & J(\\theta) = \\frac{1}{m} \\cdot \\left(-y^{T}\\log(h)-(1-y)^{T}\\log(1-h)\\right) \\newline & Repeat \\; \\lbrace \\newline & \\; \\theta_j := \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \\newline & \\rbrace \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矢量更新方式： \n",
    "\\begin{align*} & \\theta := \\theta - \\frac{\\alpha}{m} X^{T} (g(X \\theta ) - \\vec{y}) \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"mark\">注意：其和linear Regression的梯度形式一致，只不过XΘ变为了g(XΘ)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# advanced optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimization algorithms:  \n",
    "1. Gradient descent  \n",
    "2. <span class=\"mark\">conjugate gradient  \n",
    "3. <span class=\"mark\">BFGS  \n",
    "4. <span class=\"mark\">L-BFGS</span></span></span>  \n",
    "注意 ：了解这些优化算法是干什么的，里面的具体实现细节没有必要深究。如果不是专门做优化算法研究的话，只需要会用和怎么用就行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## advantages  \n",
    "1. 不需要人工选择学习率  \n",
    "2. 一般比梯度下降法要快  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## disadvantage  \n",
    "1. 更加复杂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span class=\"mark\">Octave实现优化算法流程</span>  \n",
    "1. 首先定义<span class=\"mark\">损失函数</span>，返回两个值，分别是代价函数值J(Θ)和梯度δ  \n",
    "```bash\n",
    "function [jVal, gradient] = costFunction(theta)\n",
    "  jVal = [...code to compute J(theta)...];\n",
    "  gradient = [...code to compute derivative of J(theta)...];\n",
    "end\n",
    "```\n",
    "2. 使用<span class=\"mark\">optimset()</span>，设置损失对象和最大迭代次数  \n",
    "```bash\n",
    "options = optimset('GradObj', 'on', 'MaxIter', 100);  \n",
    "```\n",
    "3. 初始化参数Θ  \n",
    "```bash\n",
    "initialTheta = zeros(2,1);  \n",
    "```\n",
    "4. 使用<span class=\"mark\">fminunc(@costF,initTheta,options)</span>，返回优化后的参数以及损失和是否收敛  \n",
    "```bash\n",
    "[optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one vs all(one-vs-rest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一对多策略  \n",
    "k个类别需要训练K个分类器，hΘ(i)  \n",
    "预测过程将测试样本代入k个分类器中，选择概率最大的作为分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a logistic regression classifier hθ(x) for each class￼ to predict the probability that ￼ ￼y = i￼ ￼.  \n",
    "To make a prediction on a new x, pick the class ￼that maximizes hθ(x)  \n",
    "![](picture/3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# solving the problem for overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reduce the number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 人工选择特征，删除可能作用较小的特征  \n",
    "2. 算法选择特征，自动选择  \n",
    "\n",
    "缺陷：可能删除的信息正是有用的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "防止模型过拟合  \n",
    "1. 保留所有的特征信息，<span class=\"mark\">但是能降低参数的量级(magnitude)?</span>可能是参数比较小。\n",
    "2. 正则化非常有用，当我们有许多有用的特征信息时。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"mark\">添加正则项，防止模型过于复杂。 \n",
    "正则项一般是所有参数的求和形式。但是，有时会去除掉偏置项，为了编程</span>方便，我会保留Θ0。  \n",
    "二者得到的结果会有细微的差别。  \n",
    "<span class=\"burk\">但是Andrew教授实验最后不正则化theta0.不知道为什么？</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 添加正则项后的损失函数形式  \n",
    "\\begin{align*} min_\\theta\\ \\dfrac{1}{2m}\\  \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda\\ \\sum_{j=1}^n \\theta_j^2 \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于logistic Regression，他的损失函数形式为：  \n",
    "\\begin{align*} J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^m \\large[ y^{(i)}\\ \\log (h_\\theta (x^{(i)})) + (1 - y^{(i)})\\ \\log (1 - h_\\theta(x^{(i)})) \\large] \\end{align*}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"mark\">添加正则项后，稍微做了修正：除上了2m，和样本数量联系在一块，似乎更合理</span>  \n",
    "\\begin{align*} & J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^m \\large[ y^{(i)}\\ \\log (h_\\theta (x^{(i)})) + (1 - y^{(i)})\\ \\log (1 - h_\\theta(x^{(i)}))\\large] + \\frac{\\lambda}{2m}\\sum_{j=1}^n \\theta_j^2  \\end{align*}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现添加的是参数的平方和，为了让参数不能太大。也是为了忽略那些可能不重要的参数，让其theta尽可能的小。  \n",
    "λ的选择：如果λ很大，惩罚模型复杂性越高，导致所有的参数趋向于0，那么最终导致的结果是拟合欠佳。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果添加的正则项不包含偏置，那么，最终除了Θ0外的所有参数趋向0，最终为一条水平线。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent  \n",
    "\\begin{align*} & \\text{Repeat}\\ \\lbrace \\newline & \\ \\ \\ \\ \\theta_0 := \\theta_0 - \\alpha\\ \\frac{1}{m}\\ \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \\newline & \\ \\ \\ \\ \\theta_j := \\theta_j - \\alpha\\ \\left[ \\left( \\frac{1}{m}\\ \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \\right) + \\frac{\\lambda}{m}\\theta_j \\right] &\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ j \\in \\lbrace 1,2...n\\rbrace\\newline & \\rbrace \\end{align*}  \n",
    "整理以后得到的更新公式为：（需要注意，Andrew Ng偏向于将Θ0偏置参数区分开，不知道为什么？）  \n",
    "\\begin{align*} & \\theta_j := \\theta_j(1 - \\alpha\\frac{\\lambda}{m}) - \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意的是： \n",
    "λα很小，而样本数m比较大，所以  \n",
    "\\begin{align*} & 0 < 1 - \\alpha\\frac{\\lambda}{m} < 1 \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normal equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}& \\theta = \\left( X^TX + \\lambda \\cdot L \\right)^{-1} X^Ty \\newline& \\text{where}\\ \\ L = \\begin{bmatrix} 0 & & & & \\newline & 1 & & & \\newline & & 1 & & \\newline & & & \\ddots & \\newline & & & & 1 \\newline\\end{bmatrix}\\end{align*}\n",
    "需要注意的是：解析法求解最优解时\\begin{align*} X^TX + \\lambda \\cdot L \\end{align*}是可逆的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 实验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. costF()\n",
    "2. fminunc() \n",
    "3. <span class=\"mark\">Regression不需要第一项</span>\n",
    "\n",
    "注意：λ需要自己设置，λ*α/m需要小于1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "369px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "810px",
    "left": "0px",
    "right": "1058px",
    "top": "106px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
