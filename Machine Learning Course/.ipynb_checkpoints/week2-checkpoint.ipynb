{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Linear Regression  \n",
    "多元线性回归：展开形式（hypothesis模型）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}h_\\theta (x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3 + \\cdots + \\theta_n x_n\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵形式：  \n",
    "\\begin{align*}h_\\theta(x) =\\begin{bmatrix}\\theta_0 \\hspace{2em} \\theta_1 \\hspace{2em} ... \\hspace{2em} \\theta_n\\end{bmatrix}\\begin{bmatrix}x_0 \\newline x_1 \\newline \\vdots \\newline x_n\\end{bmatrix}= \\theta^T x\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent method（梯度下降法求偏导形式）：  \n",
    "\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & \\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)} \\; & \\text{for j := 0...n}\\newline \\rbrace\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*} & \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_0^{(i)}\\newline \\; & \\theta_1 := \\theta_1 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_1^{(i)} \\newline \\; & \\theta_2 := \\theta_2 - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_2^{(i)} \\newline & \\cdots \\newline \\rbrace \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实用技巧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature scaling（特征缩放）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当由于特征取值范围造成偏斜过大时，轮廓图非常椭，导致的结果是收敛速度慢。  \n",
    "要让轮廓图比较圆，那么希望不同维的特征取值区间一样。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "应该说大致一样，可以接受的经验区间为[-0.5,0.5]或者[-3,3]等等，如果特征的取值范围过大或者过小，那么需要考虑将这些特征进行缩放，否则梯度下降收敛速度慢。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mean normalization（均值归一化） \n",
    "\\begin{align*}x_i := \\dfrac{x_i - \\mu_i}{s_i}\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "μi代表均值，si代表最大值和最小值的差值，最终缩放范围大致在[-0.5, 0.5]之间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convergence收敛判断"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 通常画出损失函数loss function与iteration迭代次数的关系图来判断，当损失有上升时，需要降低学习率alpha  \n",
    "2. 程序中自动判断是否收敛，在一次迭代中，如果损失函数的减少值小于10-3时，可以判断收敛，但实际上很难选，需要根据关系图来判断。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习率alpha  \n",
    "![学习率和损失函数之间的关系](picture/2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If α is too small: slow convergence.  \n",
    "If α is too large: ￼may not decrease on every iteration and thus may not converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression（多项式回归）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当trainData:{(X1,Y1),...,(Xn,Yn)}不适合线性拟合的时候，可以考虑多项式回归。  \n",
    "多项式回归和多元线性回归之间只需要做些许变换，(X1,Y1)这个训练数据只有一维，可以扩展至多元，(X1,X1^2,...,X1^m)  \n",
    "然后将其视为新的特征(x1,x2,...,xm)并作为多元线性回归特征，这样学习到的模型为多项式模型。  \n",
    "需要注意的点：由于幂的存在，导致新的特征之间取值范围相差很大，需要对新的特征进行特征缩放。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"mark\">当不适合线性拟合的时候，可以自己造特征，怎么造？之后有算法自动造？</span>  \n",
    "\\begin{align*}h_\\theta(x) = \\theta_0 + \\theta_1 x_1 \\newline h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_1^2 \\newline h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_1^2 + \\theta_3 x_1^3 \\newline h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 \\sqrt{x_1} \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Equation  \n",
    "标准方程解析式求解参数Θ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实就是最小二乘法，解析法求线性拟合的最优参数  \n",
    "适用场合：当特征的数量n不是特别大时，因为(X.T*T)-1的时间复杂度为O(n^3)，现代计算机能容忍n最大为1000左右  \n",
    "当n为上万时，比如10^6，可以考虑使用梯度下降法复杂度，<span class=\"burk\">k不知道是什么意思，是样本数量</span>O (kn2)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最小二乘法求解方程：  \n",
    "\\begin{align*} \\theta = (X^T X)^{-1}X^T y \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"burk\">怎么证明？上面的求解方程是最优解？刘老师好像有讲，但忘了。-_-||</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "189px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "810px",
    "left": "0px",
    "right": "1058px",
    "top": "106px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
