{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 聚类算法  \n",
    "- Principal Components Analysis（主成分分析）：加速学习算法，有时候非常有用，可视化及理解数据\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clustering（聚类）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "个人理解：属于<span class=\"mark\">数据挖掘</span>一类中的一部分。  \n",
    "用于挖掘没有标签<span class=\"mark\">数据的结构</span>关系，<span class=\"mark\">聚类是无监督学习的一种方法</span>。  \n",
    "可以用于比如市场分析，社交圈group，网络组织，以及其他广泛应用。 \n",
    "\n",
    "---\n",
    "<span class=\"mark\">主要的问题是：  \n",
    "- <span class=\"mark\">如何避免局部最小值：多次随机初始化K个中心值选择J最小的\n",
    "- <span class=\"mark\">如何选择K值：没有很好的方法，具体问题具体解决，可以考虑Elbow Method（肘部法则）</span></span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"mark\">K-Means算法的性质</span>：  \n",
    "- 对初值选取敏感，收敛性和收敛速度收到影响\n",
    "- 对噪声敏感\n",
    "- 不保证收敛全局最优\n",
    "\n",
    "---\n",
    "input： K个类别，训练数据X  \n",
    "output：将X分成K个类别，互不相交  \n",
    "\n",
    "---\n",
    "K-Means Algorithm  \n",
    "- 随机初始化K个聚类中心  \n",
    "- 分类，将X分为K个类，确定每一个训练样本的类别label\n",
    "- 确定新的聚类中心（计算出每一个类别的中心，然后选择离中心最近的训练样本作为新的中心）  \n",
    "- 指导聚类中心不再发生变化\n",
    "\n",
    "---\n",
    "问题：  \n",
    "- 可能在聚类过程中，有的聚类中心根本就没有元素？（如果选取训练中的数据作为聚类中心的初始值，这种情况应该不会发生，没有证明。）\n",
    "- 数据不好分，数据都已经聚在一块，没有表现得很集中？（事实上也是可以分的，只是进一步细分）\n",
    "- <span class=\"burk\">聚类数量K怎么确定？</span>\n",
    "\n",
    "---\n",
    "最简单的EM算法的运用：未知K个聚类中心在哪（Θ未知），又不知道每一个数据的label，只能先确定一者再优化另一者  \n",
    "- E步：将X按照K个聚类中心进行分类：求期望过程  \n",
    "- M步：重新确定K个聚类中心：期望最大化过程\n",
    "- 注意求解到的是局部最优解  \n",
    "- <span class=\"mark\">能够收敛，收敛性证明？</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means optimization objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K均值优化目标：  \n",
    "\n",
    "在k-means算法中，<span class=\"mark\">每一步迭代都会使得代价函数J非增</span>，可参考EM算法证明；  \n",
    "在调试k-means算法时，可以使用代价函数的变化观察算法是否正常运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解决问题：\n",
    "- <span class=\"mark\">如何避免局部最优解</span>？\n",
    "\n",
    "---\n",
    "随机化初始K个聚类中心：  \n",
    "- K<m\n",
    "- 随机选取K个训练样本  \n",
    "- 多次选取初始值多次进行训练，并选取代价函数最小的结果，避免局部收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Number of Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解决问题：   \n",
    "- <span class=\"mark\">怎么确定K值</span>？\n",
    "\n",
    "---\n",
    "解决方式：\n",
    "- 现在使用最多的方式还是手动解决，没有很好的办法\n",
    "- 可视化数据，选择合理的K值\n",
    "- <span class=\"mark\">Elbow Method</span>（肘部方法）：画出Cost function J和K值的图像，选择突变的点，就像手肘一样，突变的点  \n",
    "- Elbow Method不总是有效，可能肘部不明显\n",
    "- 考虑实际问题，做分类为了干什么，具体确定K的数值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction（维度缩减，数据压缩）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation（动机）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Compression（数据压缩）目的：\n",
    "- 重要：<span class=\"mark\">加速算法</span>  \n",
    "- 减少磁盘使用，减少存储空间\n",
    "- 降维，可视化数据，比如到2,3维\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis（PCA）主成分分析法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "去相关性，找到投影平面，将数据投影到该平面上，优化投影误差。  \n",
    "投影误差：1/m∑||x(i) - x'(i)||^2  \n",
    "\n",
    "---\n",
    "<span class=\"mark\">PCA和线性回归有区别：</span>  \n",
    "- Linear Regression：预测值和实际值之间的距离平方和开方，回归算法，有监督学习；\n",
    "- PCA：没有label，分析数据，使得变换后的数据投影误差最小，无监督学习；\n",
    "\n",
    "---\n",
    "<span class=\"mark\">课程没有证明为什么这么做能去相关性</span>\n",
    "\n",
    "---\n",
    "<span class=\"mark\">求解过程</span>：\n",
    "- <span class=\"mark\">对数据进行预处理得到X，归一化（尺度缩放）到一个合理的空间，一般减去均值除以方差</span>\n",
    "- 首先计算X的协方差矩阵：sigma = 1/m\\*X'\\*X\n",
    "- 然后计算sigma特征值，从大到小排列，再求相应特征向量(n by 1)长度为1，模为1，然后选取前K个特征向量构成变换矩阵(n by k)；\n",
    "- [U;~;~] = svd(sigma);奇异值分解\n",
    "- 做变换：Z(m,k) = X(m,n)*U(n,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# applying PCA  \n",
    "对归一化后的数据做一次PCA，保留99%的信息，通常能降维很多。  \n",
    "当然对数据PCA会丢失信息，比如可分性信息，这时可以采用Fisher变化，FDA，属于有监督学习，使得变化后的数据尽量线性可分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reconstruction from compressed representation(<span class=\"mark\">重构</span>，z->x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 通过PCA降维可以得到变换矩阵U\n",
    "- Z(mxk) = X(mxn)×U(nxk)\n",
    "- X(mxn) = Z(mxk)×U'(kxn)\n",
    "- 因为通过分解特征向量可以得到一组相互垂直（相互独立的基底），所以U'×U = eye(k)\n",
    "- U^-1的伪逆矩阵可以写成(U'U)^-1×U' = (eye(k))^-1×U' = eye(k)×U‘ = U’\n",
    "- U‘就是U的伪逆矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## choosing the number of principal components（<span class=\"mark\">怎么选择k</span>）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题：  \n",
    "- 怎么选择主成分数量K \n",
    "\n",
    "---\n",
    "- [U,S,V] = svd(sigma) \n",
    "- S是特征值矩阵，对角线元素为特征值，特征值从大到小  \n",
    "- <span class=\"mark\">计算前k个特征值和整个S矩阵对角线元素之和的比值，代表变换后矩阵保留的信息比例</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## advice for applying PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 降维，加速算法\n",
    "- 减少存储空间\n",
    "- 可视化(k = 2,3)\n",
    "\n",
    "---\n",
    "<span class=\"burk\">不建议使用PCA解决过拟合</span>：  \n",
    "PCA实际上舍弃了某些信息，使得数据的相关性降低，可能维度降为原来的十分之一，确实信息量减少了，相当于是缩减了特征数。但是这不是一个很好的方法，Dr Andrew推荐正则化，而非使用PCA。  \n",
    "\n",
    "---\n",
    "<span class=\"burk\">使用PCA的误区：</span>  \n",
    "- 不要一上来就使用PCA降维，然后训练算法；而是尝试在原始数据集上跑学习算法，看效果如何；\n",
    "- 什么时候使用？当算法<span class=\"burk\">收敛很慢</span>，或者<span class=\"burk\">内存不够</span>的时候，可以尝试使用PCA；\n",
    "- 要明白使用PCA后，算法有何不同，不要盲目，具体问题具体分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 聚类  \n",
    "- 随机初始化K个中心 \n",
    "- 计算样本到K中心的距离，选择最近的进行分类\n",
    "- 选择新的中心，均值\n",
    "- K值的选择\n",
    "- 图像压缩，颜色聚类\n",
    "\n",
    "## PCA\n",
    "- 求协方差矩阵sigma = 1/m\\*(X'\\*X)\n",
    "- 求变换矩阵svd(sigma)\n",
    "- 求变换后的数据Z = X×U\n",
    "- 数据重构 Xapprox = Z×U’"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "325px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
