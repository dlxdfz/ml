{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络受人脑工作启发，如今得到广泛的应用。  \n",
    "很古老的算法，模拟人脑的工作。  \n",
    "人大脑皮层的区域经过重新连接，能够学习到新的感知。比如使用视觉传感器传回的电压施加在舌头上，人可以尝试使用舌头”看东西“。失明人士通过回声定位等。说明大脑有自己的学习机制，是否能将这种学习机制运动在计算机上，想想就是一件很激动的事情。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当用于分类的特征空间维数很大，并且决策面表现得非常复杂时，使用logistic Regression则需要使用到分类特征的多项式组合。由于多项式组合和特征维数n是指数级关系，新组合的特征n'非常大，导致使用logistic Regression计算代价很大。所以<span class=\"burk\">不适用</span>。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![权值矩阵定义及维度，符号定义](picture/4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自学习可以用来分类的特征，输出层之前所做的事情。将特征映射到新的空间，可以用来做简单的线性分类，则逻辑回归差不多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward propagation：Vectorized implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向量化前向传播：  \n",
    "\\begin{align*}x = \\begin{bmatrix}x_0 \\newline x_1 \\newline\\cdots \\newline x_n\\end{bmatrix} &z^{(j)} = \\begin{bmatrix}z_1^{(j)} \\newline z_2^{(j)} \\newline\\cdots \\newline z_n^{(j)}\\end{bmatrix}\\end{align*}  \n",
    "\\begin{align*}z_k^{(2)} = \\Theta_{k,0}^{(1)}x_0 + \\Theta_{k,1}^{(1)}x_1 + \\cdots + \\Theta_{k,n}^{(1)}x_n \\newline a^{(j)} = g(z^{(j)}) \\newline z^{(j+1)} = \\Theta^{(j)}a^{(j)} \\newline h_\\Theta(x) = a^{(j+1)} = g(z^{(j+1)})\\end{align*}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x代表输入，z代表权值和输入乘积求和，Θ代表权值，a代表对z的激活输出。z和a的上角标代表所在层，Θ的上角标i代表从i层到i+1层的权值。向量计算步骤：  \n",
    "1. 首先为输入向量x或者a^i添加偏置1  \n",
    "2. 添加偏置后的输入向量x或者a^i和当前层到下一层的权值做内积得到z^i+1\n",
    "3. 将z^i+1代入激活哦函数得到激活输出a^i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example for xnor(x1, x2)(not xor(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](picture/5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过隐含层的特征映射，将线性不可分的特征映射到线性可分的空间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multiple output units：one-vs-all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似于逻辑回归，有几个输出则输出层包含几个神经元，代表几个分类器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regularized logistic regression's cost function  \n",
    "\\begin{align*} J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^m [ y^{(i)}\\ \\log (h_\\theta (x^{(i)})) + (1 - y^{(i)})\\ \\log (1 - h_\\theta(x^{(i)}))] + \\frac{\\lambda}{2m}\\sum_{j=1}^n \\theta_j^2\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regularized neural networks' cost function  \n",
    "\\begin{gather*} J(\\Theta) = - \\frac{1}{m} \\sum_{i=1}^m \\sum_{k=1}^K \\left[y^{(i)}_k \\log ((h_\\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\\log (1 - (h_\\Theta(x^{(i)}))_k)\\right] + \\frac{\\lambda}{2m}\\sum_{l=1}^{L-1} \\sum_{i=1}^{s_l} \\sum_{j=1}^{s_{l+1}} ( \\Theta_{j,i}^{(l)})^2\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：在逻辑回归和神经网络中，吴恩达教授不会正则化偏置参数Θ0，原意是正不正则化对结果影响不大。倾向于不正则化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BackPropagation Algorithm  \n",
    "误差反向传播，是一种与最优化算法（如梯度下降法）结合使用的，用来训练人工神经网络的常见方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BP算法：  \n",
    "![](picture/6.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 针对sigmoid激活函数具体参数更新步骤： \n",
    " \n",
    "Given training set {(x(1),y(1))⋯(x(m),y(m))}  \n",
    "Set Δ(l)i,j := 0 for all (l,i,j), (hence you end up having a matrix full of zeros)  \n",
    "For training example t =1 to m:  \n",
    "% 代表每一个训练样本  \n",
    "1. 前向传播计算每一层输出a(layer)\n",
    "Set a(1):=x(t)  \n",
    "Perform forward propagation to compute a(l) for l=2,3,…,L  \n",
    "2. 更新梯度   \n",
    "2.1 计算δ（理解为梯度的中间值，递归计算各个层的梯度）  \n",
    "Using y(t), compute δ(L)=a(L)−y(t)  \n",
    "然后计算δ(L−1),δ(L−2),…,δ(2)  \n",
    "使用的更新公式为：\\begin{align*} \\delta^{(l)} = ((\\Theta^{(l)})^T \\delta^{(l+1)})\\ .*\\ a^{(l)}\\ .*\\ (1 - a^{(l)}) \\end{align*}  \n",
    "2.2 计算Δ  \n",
    "\\begin{align*} \\Delta^{(l)}_{i,j} := \\Delta^{(l)}_{i,j} + a_j^{(l)} \\delta_i^{(l+1)} \\newline \\text 矢量化为：\\Delta^{(l)} := \\Delta^{(l)} + \\delta^{(l+1)}(a^{(l)})^T\\end{align*}  \n",
    "2.3 计算梯度D  \n",
    "\\begin{align*} D^{(l)}_{i,j} := \\dfrac{1}{m}\\left(\\Delta^{(l)}_{i,j} + \\lambda\\Theta^{(l)}_{i,j}\\right) \\text if  j≠0\\newline D^{(l)}_{i,j} := \\dfrac{1}{m}\\Delta^{(l)}_{i,j} \\text if  j=0\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# backpropagation in practice  \n",
    "因为在具体使用优化方式拟合参数的时候，需要对向量的形式做相应的变化。  \n",
    "以Octave优化方法fminunc()为例：她优化的参数和所需的梯度都是向量形式n*1的形式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unrolling parameters  \n",
    "将参数从矩阵转换为向量  \n",
    "将梯度结果也展成向量的形式  \n",
    "D1，D2，D3 -> deltavc  \n",
    "thetaVector = [ Theta1(:); Theta2(:); Theta3(:); ]  \n",
    "deltaVector = [ D1(:); D2(:); D3(:) ]  \n",
    "\n",
    "从unrolling parameters变为之前的矩阵方法    \n",
    "Theta1 = reshape(thetaVector(1:110),10,11)  \n",
    "Theta2 = reshape(thetaVector(111:220),10,11)  \n",
    "Theta3 = reshape(thetaVector(221:231),1,11)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradient checking  \n",
    "为了验证自己的梯度后向传播公式推导是否正确，需要进行数值计算验证  \n",
    "\\begin{align*} \\dfrac{\\partial}{\\partial\\Theta}J(\\Theta) \\approx \\dfrac{J(\\Theta + \\epsilon) - J(\\Theta - \\epsilon)}{2\\epsilon} \\newline \\dfrac{\\partial}{\\partial\\Theta_j}J(\\Theta) \\approx \\dfrac{J(\\Theta_1, \\dots, \\Theta_j + \\epsilon, \\dots, \\Theta_n) - J(\\Theta_1, \\dots, \\Theta_j - \\epsilon, \\dots, \\Theta_n)}{2\\epsilon}\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更新过程：  \n",
    "```bash  \n",
    "epsilon = 1e-4;  \n",
    "for i = 1:n,  \n",
    "  thetaPlus = theta;  \n",
    "  thetaPlus(i) += epsilon;  \n",
    "  thetaMinus = theta;  \n",
    "  thetaMinus(i) -= epsilon;  \n",
    "  gradApprox(i) = (J(thetaPlus) - J(thetaMinus))/(2*epsilon)  \n",
    "end;  \n",
    "```  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 为什么使用梯度下降而非近似数值计算偏导？  \n",
    "数值计算代价太高。  \n",
    "2. epsilon选择？  \n",
    "经验值： ϵ=10^−4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random initialization  \n",
    "随机初始化参数，初始化的参数尽量小，趋近于0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果所有的参数都初始化为0，考虑简单的三层模型，会发现所有的隐含层输出都相同，并且隐含层除了偏置以外的节点计算的梯度都相同。可想而知在之后的更新中，从输入到隐含层的参数都一样，结果是隐含节点输出一样，造成很大的冗余，因为隐含层输入的有用信息其实只有一维特征，网络性能退化。<span class=\"mark\">为了打破这种对称性，’symmetry breaking‘</span>，这时可以使用随机初始化参数。 \n",
    "```bash  \n",
    "If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11.  \n",
    "\n",
    "Theta1 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;  \n",
    "Theta2 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;  \n",
    "Theta3 = rand(1,11) * (2 * INIT_EPSILON) - INIT_EPSILON;  \n",
    "```  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络训练步骤总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pick a  network architecture  \n",
    "1. 总共需要多少层，以及隐含层需要包含的节点数目。\n",
    "2. 输入层节点数目和特征数相同。  \n",
    "3. 输出层节点数目和类别数相同。  \n",
    "4. 每一个隐含层通常节点数相同，并且越多性能越好，计算代价也越高。通常和输入层相同，或者略多，2~4倍。\n",
    "5. 隐含层层数：通常是一层。如果多于一层，建议每一层包含的节点数相同。  \n",
    "6. <span class=\"burk\">3层的神经网络性能已经表现得很好，增加网络层数不能提升网络的性能？为什么。--金野</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training a neural network  \n",
    "1. 随机初始化权值，symmetry breaking  <span class=\"mark\">epsilon=sqrt(6)/(sqrt(s(l)) + (s(l+1)))在实验说明文档上</span>  \n",
    "2. 对每一个训练样本做前向传播计算。  \n",
    "3. 计算损失函数。  \n",
    "4. 反向传播计算梯度。（记得梯度和参数都是向量形式4.1）  \n",
    "5. 使用梯度检测确定反向传播的准确性。然后关闭梯度检测。（数值计算）<span class=\"mark\">epsilon = 10^-4</span>   \n",
    "6. 使用梯度下降或者其他优化算法最小化损失函数。得到最优化参数。  \n",
    "\n",
    "在进行梯度反向传播时，最好使用for循环为每一个样本计算梯度：  \n",
    "```bash\n",
    "for i = 1:m,\n",
    "   Perform forward propagation and backpropagation using example (x(i),y(i))  \n",
    "   (Get activations a(l) and delta terms d(l) for l = 2,...,L  \n",
    "```\n",
    "当然也可以不使用for循环，使用更高级的实现方式。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算梯度和损失函数Octave实现  \n",
    "```bash  \n",
    "function [J grad] = nnCostFunction(nn_params, ...  \n",
    "                                   input_layer_size, ...  \n",
    "                                   hidden_layer_size, ...  \n",
    "                                   num_labels, ...  \n",
    "                                   X, y, lambda)  \n",
    "%NNCOSTFUNCTION Implements the neural network cost function for a two layer  \n",
    "%neural network which performs classification  \n",
    "%   [J grad] = NNCOSTFUNCTON(nn_params, hidden_layer_size, num_labels, ...  \n",
    "%   X, y, lambda) computes the cost and gradient of the neural network. The  \n",
    "%   parameters for the neural network are \"unrolled\" into the vector  \n",
    "%   nn_params and need to be converted back into the weight matrices.   \n",
    "%   \n",
    "%   The returned parameter grad should be a \"unrolled\" vector of the  \n",
    "%   partial derivatives of the neural network.  \n",
    "%  \n",
    "\n",
    "% Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices  \n",
    "% for our 2 layer neural network  \n",
    "Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...  \n",
    "                 hidden_layer_size, (input_layer_size + 1));  \n",
    "\n",
    "Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...  \n",
    "                 num_labels, (hidden_layer_size + 1));  \n",
    "\n",
    "% Setup some useful variables  \n",
    "m = size(X, 1);  \n",
    "           \n",
    "% You need to return the following variables correctly   \n",
    "J = 0;  \n",
    "Theta1_grad = zeros(size(Theta1));  \n",
    "Theta2_grad = zeros(size(Theta2));  \n",
    "\n",
    "% ====================== YOUR CODE HERE ======================  \n",
    "% Instructions: You should complete the code by working through the  \n",
    "%               following parts.  \n",
    "%  \n",
    "% Part 1: Feedforward the neural network and return the cost in the  \n",
    "%         variable J. After implementing Part 1, you can verify that your  \n",
    "%         cost function computation is correct by verifying the cost  \n",
    "%         computed in ex4.m  \n",
    "%  \n",
    "% Part 2: Implement the backpropagation algorithm to compute the gradients  \n",
    "%         Theta1_grad and Theta2_grad. You should return the partial derivatives of  \n",
    "%         the cost function with respect to Theta1 and Theta2 in Theta1_grad and  \n",
    "%         Theta2_grad, respectively. After implementing Part 2, you can check  \n",
    "%         that your implementation is correct by running checkNNGradients  \n",
    "%  \n",
    "%         Note: The vector y passed into the function is a vector of labels  \n",
    "%               containing values from 1..K. You need to map this vector into a   \n",
    "%               binary vector of 1's and 0's to be used with the neural network  \n",
    "%               cost function.  \n",
    "%  \n",
    "%         Hint: We recommend implementing backpropagation using a for-loop  \n",
    "%               over the training examples if you are implementing it for the   \n",
    "%               first time.  \n",
    "%  \n",
    "% Part 3: Implement regularization with the cost function and gradients.  \n",
    "%  \n",
    "%         Hint: You can implement this around the code for  \n",
    "%               backpropagation. That is, you can compute the gradients for  \n",
    "%               the regularization separately and then add them to Theta1_grad  \n",
    "%               and Theta2_grad from Part 2.  \n",
    "%  \n",
    "% feedforward  \n",
    "% 三层网络做前向传播  \n",
    "a1 = [ones(m, 1), X];  \n",
    "z2 = a1*Theta1';  \n",
    "a2 = sigmoid(z2);  \n",
    "a2 = [ones(m, 1), a2];  \n",
    "z3 = a2*Theta2';  \n",
    "a3 = sigmoid(z3);      % m by K  \n",
    "\n",
    "% compute cost without regularization  \n",
    "% y_matrix quckily init? \n",
    "% 将目标输出由1,2,3,4,5,6,7,8,9,10的形式变为0,1向量形式  \n",
    "y_matrix = zeros(m, num_labels);  \n",
    "for i = 1:m  \n",
    "  y_matrix(i, y(i)) = 1;  \n",
    "end  \n",
    "% 损失函数计算  \n",
    "J = -1/m * sum(sum(y_matrix.*log(a3) + (1-y_matrix).*log(1 - a3)));  \n",
    "\n",
    "% compute cost with regularization  \n",
    "% 添加正则项\n",
    "J_reg = lambda/(2*m) * (sum(sum(Theta1(:, 2:end).^2)) + sum(sum(Theta2(:, 2:end).^2)));%not contain   theta0\n",
    "J += J_reg;  \n",
    "\n",
    "% backpropagation algorithm implement without regularization  \n",
    "% 误差反向传播\n",
    "delta1 = zeros(size(Theta1));  \n",
    "delta2 = zeros(size(Theta2));  \n",
    "% 累计m个训练样本的梯度  \n",
    "for t = 1:m  \n",
    "  delta_3 = a3(t, :)' - y_matrix(t, :)';                     % K by 1  \n",
    "  delta_2 = Theta2'*delta_3.*sigmoidGradient([1, z2(t, :)]');% (s2 + 1) by 1  \n",
    "  delta2 += delta_3*a2(t, :);                               % K by (s2 + 1)  \n",
    "  delta1 += delta_2(2: end)*a1(t, :);                       % s2 by n  \n",
    "end  \n",
    "% 取梯度的均值  \n",
    "Theta1_grad = 1/m * delta1;  \n",
    "Theta2_grad = 1/m * delta2;  \n",
    "\n",
    "% backpropagation algorithm implement with regularization  \n",
    "% 添加正则项梯度  \n",
    "Theta1(:, 1) = 0;  \n",
    "Theta1_grad += lambda/m * Theta1;  \n",
    "Theta2(:, 1) = 0;  \n",
    "Theta2_grad += lambda/m * Theta2;  \n",
    "\n",
    "\n",
    "% -------------------------------------------------------------  \n",
    "\n",
    "% =========================================================================  \n",
    "\n",
    "% Unroll gradients  \n",
    "% unrolling为向量形式\n",
    "grad = [Theta1_grad(:) ; Theta2_grad(:)];  \n",
    "\n",
    "\n",
    "end  \n",
    "\n",
    "```  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从训练到测试整个过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash  \n",
    "% load data  \n",
    "% 加载数据X,y\n",
    "load('ex4data1.mat');  \n",
    "\n",
    "% define parameters  \n",
    "input_layer_size = 400;  \n",
    "hidden_layer_size = 25;  \n",
    "num_labels = 10;  \n",
    "theta1 = randInitializeWeights(input_layer_size, hidden_layer_size); % 25 by 401  \n",
    "theta2 = randInitializeWeights(hidden_layer_size, num_labels);  % 10 by 26  \n",
    "initial_nn_params = [theta1(:); theta2(:)];  \n",
    "\n",
    "% fmincg optimal parameters  \n",
    "%  After you have completed the assignment, change the MaxIter to a larger  \n",
    "%  value to see how more training helps.  \n",
    "options = optimset('MaxIter', 50);  \n",
    "%  You should also try different values of lambda  \n",
    "lambda = 1;  \n",
    "% Create \"short hand\" for the cost function to be minimized  \n",
    "costFunction = @(p) nnCostFunction(p, ...  \n",
    "                                   input_layer_size, ...  \n",
    "                                   hidden_layer_size, ...  \n",
    "                                   num_labels, X, y, lambda);  \n",
    "% Now, costFunction is a function that takes in only one argument (the  \n",
    "% neural network parameters)  \n",
    "[nn_params, cost] = fmincg(costFunction, initial_nn_params, options);  \n",
    "\n",
    "% predict   \n",
    "theta1 = reshape(nn_params(1 : (input_layer_size+1)*hidden_layer_size ), [hidden_layer_size, input_layer_size+1]);\n",
    "theta2 = reshape(nn_params((input_layer_size+1)*hidden_layer_size+1 : end), [num_labels, hidden_layer_size  +1]); \n",
    "p = predict(theta1, theta2, X);  \n",
    "correct = size(find(p == y), 1)/size(X, 1);  \n",
    "```  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "225px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "810px",
    "left": "0px",
    "right": "1058px",
    "top": "106px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
